{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#@title üîë Clusterizza e Misura (speciale AST 2025)\n",
        "\n",
        "[![Licenza MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFqTdqcdOAKj",
        "outputId": "279e5ac5-3690-4674-bb2e-0b287cf8ea63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dipendenze installate.\n"
          ]
        }
      ],
      "source": [
        "# FONDAMENTALE!\n",
        "# Installiamo in memoria le librerie necessarie\n",
        "# NB: nelle macchine virtuali di colab √® necessario installare ogni volta le librerie se la macchina √® stata riavviata o si √® auto-disattivata\n",
        "!pip install requests pandas -q\n",
        "print(\"Dipendenze installate.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0bYULzRORU2",
        "outputId": "df938da5-74e3-4866-af03-3cc6e93949e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Campo di inserimento della chiave API elaborato. L'assegnazione della chiave avviene nella cella successiva.\n"
          ]
        }
      ],
      "source": [
        "#@title üîë Inserisci la chiave API di OpenRouter (o usa i Segreti di Colab)\n",
        "#@markdown ---\n",
        "#@markdown **Questa cella ti permette di fornire la tua chiave API di OpenRouter.**\n",
        "#@markdown\n",
        "#@markdown **Opzione 1 (Consigliata & Sicura): Usa i Segreti di Colab**\n",
        "#@markdown 1. Clicca sull'icona 'üîë' (Chiave) nella barra laterale sinistra.\n",
        "#@markdown 2. Aggiungi un nuovo segreto chiamato `OPENROUTER_API_KEY`.\n",
        "#@markdown 3. Incolla la tua chiave API come valore e abilita \"Accesso al notebook\".\n",
        "#@markdown 4. **Lascia il campo di input qui sotto VUOTO.** Lo script user√† automaticamente il segreto.\n",
        "#@markdown ---\n",
        "#@markdown **Opzione 2 (Meno Sicura - Incolla Direttamente):**\n",
        "#@markdown Se preferisci non usare i Segreti di Colab, incolla direttamente la tua chiave API nel campo qui sotto.\n",
        "#@markdown **Nota:** Chiunque condivida questo notebook con te e esegua questa cella potrebbe vedere la chiave se ispeziona l'output salvato.\n",
        "api_key_from_form = \"la_tua_chiave_api\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "print(\"Campo di inserimento della chiave API elaborato. L'assegnazione della chiave avviene nella cella successiva.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JO6WICjnOxlm"
      },
      "outputs": [],
      "source": [
        "#@title Prompts\n",
        "\n",
        "# il classification prompt √® quello che serve per classificare le query in base alla categoria e alla sotto categoria\n",
        "# il similarity prompt √® quello che serve per misurare la somiglianza semantica tra il risultato dell'LLM e la ground truth letta da file\n",
        "\n",
        "# se dovete cambiare i prompt, potete farlo qui sotto, per ottenere un risultato soddisfacente dovete definire categorie e sotto categorie che siano appropriate per il vostro dataset\n",
        "\n",
        "CLASSIFICATION_PROMPT_TEMPLATE = \"\"\"Classify a series of queries related to tourism in Italy for the year 2025.\n",
        "Assign each query only one of the most appropriate category from the list provided, then assign Only one of the most appropriate subcategory from the list.\n",
        "If the term is not Italian, interpret the meaning and choose the closest category and subcategory listed.\n",
        "Categories\n",
        "\t-\tCantine\n",
        "\t-\tDegustazione formaggi\n",
        "\t-\tDegustazione grappe\n",
        "\t-\tDegustazione miele\n",
        "\t-\tDegustazione vini\n",
        "\t-\tFrantoi\n",
        "\t-\tServizi\n",
        "\t-\tTurismo enogastronomico\n",
        "Subcategories\n",
        "\t-\tFranciacorta\n",
        "\t-\tLago di Garda\n",
        "\t-\tTrentino Alto Adige\n",
        "\t-\tGenerico\n",
        "\t-\tOnline\n",
        "Output format\n",
        "category,subcategory\n",
        "(do not add additional context!)\n",
        "\n",
        "\n",
        "Examples, (ground truth)\n",
        "\t- cantina franciacorta visita > Cantine,Franciacorta\n",
        "\t- degustazione vini in bicicletta > Degustazione vini,Generico\n",
        "\t- degustazione vini sirmione > Degustazione vini,Lago di Garda\n",
        "\t- weekend enogastronomici > Turismo enogastronomico,Generico\n",
        "query: {input_text}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "SIMILARITY_PROMPT_TEMPLATE = \"\"\"\n",
        "Abbiamo fatto la classificazione di una query.\n",
        "Ti fornisco il risultato ottenuto dall'LLM e la classificazione corretta (ground truth).\n",
        "Misura la somiglianza semantica tra questi due testi.\n",
        "Voglio un punteggio per capire quanto il risultato dell'LLM corrisponde alla ground truth.\n",
        "Esprimi **solo** un punteggio numerico compreso tra 0 (completamente dissimile) e 100 (semanticamente identico), senza aggiungere alcuna spiegazione o testo aggiuntivo.\n",
        "\n",
        "Ground Truth: \"{ground_truth_text}\"\n",
        "Risultato LLM: \"{llm_result_text}\"\n",
        "\n",
        "Punteggio (0-100):\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5bxVg5NVje0",
        "outputId": "40360703-121d-471d-d957-fbd1eb8d918f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ìò API Key Secret 'OPENROUTER_API_KEY' not found. Checking form input...\n",
            "‚úÖ API Key loaded from the direct input form field.\n",
            "   ‚ö†Ô∏è Warning: Using the form field is less secure than Colab Secrets.\n",
            "üîë Using API Key obtained from: Direct Input Form\n",
            "\n",
            "üìÑ Carica il tuo file CSV di input (ad esempio, 'input.csv').\n",
            "   Deve contenere obbligatoriamente le colonne chiamate 'input' e 'ground_truth'.\n",
            "   La colonna 'input' deve contenere le query da classificare.\n",
            "   La colonna 'ground_truth' deve contenere le categorie e sotto categorie identificate dal professionista (vedi prompt).\n",
            "   ‚úÖ Usa la scheda 'File' (icona a forma di cartella) nel pannello a sinistra per caricare il file.\n"
          ]
        }
      ],
      "source": [
        "#@title 3.Importiamo le chiavi API e i settaggi\n",
        "\n",
        "# questo passaggio √® per rendere il sistema pi√π robusto e sicuro, ma per dei testi privati √® sufficiente inserire la chiave API direttamente nella cella 2\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from google.colab import userdata # For Colab secrets\n",
        "from google.colab import files    # For file uploads/downloads\n",
        "\n",
        "# --- Determine the Final API Key ---\n",
        "FINAL_API_KEY = None\n",
        "api_key_source = \"None\"\n",
        "\n",
        "# 1. Try Colab Secrets first\n",
        "try:\n",
        "    key_from_secrets = userdata.get('OPENROUTER_API_KEY')\n",
        "    if key_from_secrets:\n",
        "        FINAL_API_KEY = key_from_secrets\n",
        "        api_key_source = \"Colab Secrets\"\n",
        "        print(\"‚úÖ API Key loaded successfully from Colab Secrets.\")\n",
        "    else:\n",
        "         print(\"‚ìò API Key Secret 'OPENROUTER_API_KEY' found but is empty. Checking form input...\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"‚ìò API Key Secret 'OPENROUTER_API_KEY' not found. Checking form input...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error accessing Colab Secrets: {e}. Checking form input...\")\n",
        "\n",
        "# 2. Fallback to form input if secrets didn't yield a key\n",
        "if FINAL_API_KEY is None:\n",
        "    if 'api_key_from_form' in locals():\n",
        "        key_input_stripped = api_key_from_form.strip()\n",
        "        if key_input_stripped:\n",
        "            FINAL_API_KEY = key_input_stripped\n",
        "            api_key_source = \"Direct Input Form\"\n",
        "            print(\"‚úÖ API Key loaded from the direct input form field.\")\n",
        "            print(\"   ‚ö†Ô∏è Warning: Using the form field is less secure than Colab Secrets.\")\n",
        "        else:\n",
        "             print(\"‚ìò Direct input form field is also empty.\")\n",
        "    else:\n",
        "         print(\"Error: api_key_from_form variable not found. Ensure Cell 2 ran.\")\n",
        "\n",
        "# 3. Final Check\n",
        "if FINAL_API_KEY:\n",
        "    print(f\"üîë Using API Key obtained from: {api_key_source}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: OpenRouter API Key is MISSING.\")\n",
        "    print(\"   Please provide it via Colab Secrets or the form in Cell 2.\")\n",
        "\n",
        "# --- Istruzioni per il caricamento del file ---\n",
        "print(\"\\nüìÑ Carica il tuo file CSV di input (ad esempio, 'input.csv').\")\n",
        "print(\"   Deve contenere obbligatoriamente le colonne chiamate 'input' e 'ground_truth'.\")\n",
        "print(\"   La colonna 'input' deve contenere le query da classificare.\")\n",
        "print(\"   La colonna 'ground_truth' deve contenere le categorie e sotto categorie identificate dal professionista (vedi prompt).\")\n",
        "print(\"   ‚úÖ Usa la scheda 'File' (icona a forma di cartella) nel pannello a sinistra per caricare il file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYFphzGeVxwG",
        "outputId": "b06def32-a52e-474a-a655-16e74e2af0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ La configurazione sembra corretta.\n",
            "\n",
            "--- Impostazioni attuali ---\n",
            "Modello di classificazione: openrouter/horizon-beta\n",
            "Modello di similarit√†: openai/gpt-4o-mini\n",
            "CSV di input: /content/test-300-martino.csv\n",
            "CSV di output: /content/evaluation_results_horizonbeta.csv\n",
            "Attesa per riga: 0.3s\n",
            "Prompt impostati (visualizzabili nel form sopra).\n"
          ]
        }
      ],
      "source": [
        "#@title 4. Configure Models, Prompts, Files & Delay\n",
        "\n",
        "# --- LLM Models ---\n",
        "CLASSIFICATION_MODEL = \"openai/gpt-5.1-chat\" #@param {type:\"string\"}\n",
        "SIMILARITY_MODEL = \"openai/gpt-4o-mini\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# --- File Paths ---\n",
        "# Assumes files are uploaded to the root Colab directory '/content/'\n",
        "INPUT_CSV_PATH = \"/content/test-300-martino.csv\" #@param {type:\"string\"}\n",
        "OUTPUT_CSV_PATH = \"/content/evaluation_results_gpt51chat.csv\" #@param {type:\"string\"}\n",
        "\n",
        "# --- Processing Delay ---\n",
        "DELAY_SECONDS = 0.2 #@param {type:\"number\"}\n",
        "\n",
        "# --- Temperature---\n",
        "TEMPERATURE = 0 #@param {type:\"number\"}\n",
        "#@markdown ‚ö†Ô∏è **Nota:** Se la temperatura √® impostata a 0, il parametro verr√† omesso in fase di chiamata API.\n",
        "# scriviamo sotto il parametro TEMPERATURE  un messaggio che se la temperatura √® 0, il parametro viene omesso\n",
        "if TEMPERATURE == 0:\n",
        "    print(\"‚ö†Ô∏è nota bene: la temperatura √® impostata a 0, il parametro verr√† omesso in fase di chiamata API.\")\n",
        "\n",
        "\n",
        "# --- API Endpoint (Usually fixed) ---\n",
        "OPENROUTER_API_ENDPOINT = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "# --- Validation & Summary ---\n",
        "errors = []\n",
        "if not CLASSIFICATION_MODEL: errors.append(\"CLASSIFICATION_MODEL cannot be empty.\")\n",
        "if not SIMILARITY_MODEL: errors.append(\"SIMILARITY_MODEL cannot be empty.\")\n",
        "if \"{input_text}\" not in CLASSIFICATION_PROMPT_TEMPLATE: errors.append(\"CLASSIFICATION_PROMPT_TEMPLATE missing '{input_text}'.\")\n",
        "if \"{ground_truth_text}\" not in SIMILARITY_PROMPT_TEMPLATE: errors.append(\"SIMILARITY_PROMPT_TEMPLATE missing '{ground_truth_text}'.\")\n",
        "if \"{llm_result_text}\" not in SIMILARITY_PROMPT_TEMPLATE: errors.append(\"SIMILARITY_PROMPT_TEMPLATE missing '{llm_result_text}'.\")\n",
        "if not INPUT_CSV_PATH: errors.append(\"INPUT_CSV_PATH cannot be empty.\")\n",
        "if not OUTPUT_CSV_PATH: errors.append(\"OUTPUT_CSV_PATH cannot be empty.\")\n",
        "if DELAY_SECONDS < 0: errors.append(\"DELAY_SECONDS cannot be negative.\")\n",
        "\n",
        "if errors:\n",
        "    print(\"‚ùå Configuration Errors Found:\")\n",
        "    for err in errors:\n",
        "        print(f\"   - {err}\")\n",
        "else:\n",
        "    print(\"‚úÖ Configuration looks OK.\")\n",
        "    print(\"\\n--- Current Settings ---\")\n",
        "    print(f\"Classification Model: {CLASSIFICATION_MODEL}\")\n",
        "    print(f\"Similarity Model: {SIMILARITY_MODEL}\")\n",
        "    print(f\"Input CSV: {INPUT_CSV_PATH}\")\n",
        "    print(f\"Output CSV: {OUTPUT_CSV_PATH}\")\n",
        "    print(f\"Delay per row: {DELAY_SECONDS}s\")\n",
        "    print(f\"Temperature: {TEMPERATURE}\")\n",
        "    # Displaying prompts might be long, consider commenting out if needed\n",
        "    # print(\"Classification Prompt:\\n```\\n\" + CLASSIFICATION_PROMPT_TEMPLATE + \"\\n```\")\n",
        "    # print(\"Similarity Prompt:\\n```\\n\" + SIMILARITY_PROMPT_TEMPLATE + \"\\n```\")\n",
        "    print(\"Prompts set (view in form above).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title 4b. Configurazione Reasoning e Parametri Aggiuntivi\n",
        "#@markdown ---\n",
        "#@markdown **Controllo Reasoning Tokens per modelli OpenRouter**\n",
        "#@markdown\n",
        "#@markdown Configura i parametri di reasoning. **IMPORTANTE**: OpenRouter non permette di usare entrambi i parametri insieme.\n",
        "#@markdown - **Max Tokens**: Per modelli Anthropic/Gemini - numero di token per il reasoning (0 = disabilitato)\n",
        "#@markdown - **Effort Level**: Per modelli OpenAI (o1, o3, GPT-5) e Grok - livello di intensit√† (\"none\" = disabilitato)\n",
        "#@markdown\n",
        "#@markdown ‚ö†Ô∏è **Nota**: Se imposti entrambi, verr√† usato solo \"Effort Level\" (priorit√† ai modelli OpenAI/Grok).\n",
        "#@markdown\n",
        "#@markdown üí° **Suggerimento**: Per disabilitare il reasoning su Grok/OpenAI, usa `effort: \"none\"`.\n",
        "#@markdown Se usi `max_tokens: 0`, verr√† automaticamente convertito in `effort: \"none\"` per questi modelli.\n",
        "#@markdown\n",
        "#@markdown üìñ [Documentazione OpenRouter Reasoning](https://openrouter.ai/docs/use-cases/reasoning-tokens)\n",
        "#@markdown ---\n",
        "\n",
        "import json\n",
        "#@markdown ---\n",
        "\n",
        "# --- Configurazione Max Tokens (per modelli Anthropic/Gemini) ---\n",
        "REASONING_MAX_TOKENS = 30 #@param {type:\"raw\"}\n",
        "#@markdown **Max Tokens per Reasoning** (lascia vuoto per usare il default, oppure imposta un numero):\n",
        "#@markdown - Lascia vuoto: usa la configurazione predefinita (max_tokens: 25)\n",
        "#@markdown - Numero > 0: specifica il numero massimo di token per il reasoning\n",
        "#@markdown - 0: disabilita il reasoning\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# --- Configurazione Effort Level (per modelli OpenAI) ---\n",
        "REASONING_EFFORT = \"none\" #@param [ \"high\", \"medium\", \"low\", \"minimal\", \"none\"]\n",
        "#@markdown **Effort Level** (lascia \"None\" per non usare questo parametro):\n",
        "#@markdown - `high`: ~80% dei max_tokens per reasoning\n",
        "#@markdown - `medium`: ~50% dei max_tokens per reasoning\n",
        "#@markdown - `low`: ~20% dei max_tokens per reasoning\n",
        "#@markdown - `minimal`: ~10% dei max_tokens per reasoning\n",
        "#@markdown - `none`: Disabilita il reasoning\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# --- Escludi reasoning dalla risposta ---\n",
        "REASONING_EXCLUDE = False #@param {type:\"boolean\"}\n",
        "#@markdown **Escludi reasoning dalla risposta:**\n",
        "#@markdown Se `True`, il modello user√† reasoning internamente ma non lo restituir√† nella risposta.\n",
        "\n",
        "# --- Costruzione del dizionario parametri ---\n",
        "ADDITIONAL_PARAMS_DICT = None\n",
        "reasoning_config = {}\n",
        "\n",
        "# IMPORTANTE: OpenRouter non permette di specificare sia \"effort\" che \"max_tokens\" insieme\n",
        "# Se entrambi sono impostati, diamo priorit√† a \"effort\" (pi√π comune per modelli OpenAI)\n",
        "\n",
        "has_effort = REASONING_EFFORT is not None and REASONING_EFFORT != \"None\" and REASONING_EFFORT != \"\"\n",
        "has_max_tokens = REASONING_MAX_TOKENS is not None and REASONING_MAX_TOKENS != \"\"\n",
        "\n",
        "# Gestione effort (ha priorit√† se entrambi sono impostati)\n",
        "if has_effort:\n",
        "    reasoning_config[\"effort\"] = REASONING_EFFORT\n",
        "    if has_max_tokens:\n",
        "        print(\"‚ö†Ô∏è Nota: sia 'effort' che 'max_tokens' sono impostati. Verr√† usato solo 'effort' (OpenRouter non permette entrambi).\")\n",
        "\n",
        "# Gestione max_tokens (solo se effort non √® impostato)\n",
        "elif has_max_tokens:\n",
        "    try:\n",
        "        max_tokens_val = int(REASONING_MAX_TOKENS)\n",
        "        reasoning_config[\"max_tokens\"] = max_tokens_val\n",
        "    except (ValueError, TypeError):\n",
        "        print(\"‚ö†Ô∏è Warning: REASONING_MAX_TOKENS non √® un numero valido. Verr√† ignorato.\")\n",
        "\n",
        "# Gestione exclude (pu√≤ essere aggiunto a qualsiasi configurazione)\n",
        "if REASONING_EXCLUDE:\n",
        "    reasoning_config[\"exclude\"] = True\n",
        "\n",
        "# Costruiamo il dizionario finale solo se abbiamo almeno un parametro\n",
        "if reasoning_config:\n",
        "    ADDITIONAL_PARAMS_DICT = {\"reasoning\": reasoning_config}\n",
        "\n",
        "    # Messaggi informativi\n",
        "    if \"max_tokens\" in reasoning_config:\n",
        "        if reasoning_config[\"max_tokens\"] == 0:\n",
        "            print(\"‚úÖ Reasoning disabilitato (max_tokens: 0)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Reasoning configurato: max_tokens={reasoning_config['max_tokens']}\")\n",
        "    elif \"effort\" in reasoning_config:\n",
        "        if reasoning_config[\"effort\"] == \"none\":\n",
        "            print(\"‚úÖ Reasoning disabilitato (effort: 'none')\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Reasoning configurato: effort='{reasoning_config['effort']}'\")\n",
        "\n",
        "    if REASONING_EXCLUDE:\n",
        "        print(\"   Reasoning tokens esclusi dalla risposta.\")\n",
        "else:\n",
        "    print(\"‚ìò Usando la configurazione predefinita del reasoning (max_tokens: 25).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TBbLNV7WYEc",
        "outputId": "af94795a-9635-4ea5-f541-c085e9b81a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions defined.\n"
          ]
        }
      ],
      "source": [
        "#@title 5. Define Helper Functions\n",
        "\n",
        "def deep_merge_dict(base_dict, update_dict):\n",
        "    \"\"\"\n",
        "    Merge update_dict into base_dict recursively.\n",
        "    Nested dictionaries are merged, not replaced.\n",
        "    \"\"\"\n",
        "    result = base_dict.copy()\n",
        "    for key, value in update_dict.items():\n",
        "        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
        "            result[key] = deep_merge_dict(result[key], value)\n",
        "        else:\n",
        "            result[key] = value\n",
        "    return result\n",
        "\n",
        "def call_openrouter_llm(messages, model_name, api_key, purpose=\"task\", timeout=60):\n",
        "    \"\"\"Generic function to call the OpenRouter Chat Completions API.\"\"\"\n",
        "    if not api_key: # Check moved inside\n",
        "        print(f\"\\n‚ùå Error ({purpose}): OpenRouter API Key is missing.\")\n",
        "        return None\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        # Add Referer/Title if needed for OpenRouter tracking\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "        \"X-Title\": \"Colab LLM Eval\",\n",
        "    }\n",
        "    max_tokens = 500 if purpose == \"classification\" else 550\n",
        "    # Determine temperature based on purpose, but be mindful of model compatibility\n",
        "    current_temperature = TEMPERATURE if purpose == \"classification\" else 0.1\n",
        "\n",
        "    # Updated data structure to potentially leverage caching and reasoning\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": messages, # Use the structured messages passed from calling function\n",
        "        \"max_tokens\": max_tokens,\n",
        "    }\n",
        "\n",
        "    # Default reasoning configuration (used if section 4b doesn't override it)\n",
        "    default_reasoning = {\n",
        "        \"reasoning\": {\n",
        "            \"max_tokens\": 25\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Conditionally add temperature if it's greater than 0, as some models don't support it\n",
        "    # NOTE: This is a heuristic. A more robust solution might involve checking model capabilities\n",
        "    # via the OpenRouter API itself, but for now, we only add temperature if it's non-zero.\n",
        "    if current_temperature > 0:\n",
        "        data[\"temperature\"] = current_temperature\n",
        "\n",
        "    # Add reasoning and additional parameters from section 4b if configured\n",
        "    if 'ADDITIONAL_PARAMS_DICT' in globals() and ADDITIONAL_PARAMS_DICT is not None:\n",
        "        # If ADDITIONAL_PARAMS_DICT contains reasoning, it replaces the default\n",
        "        # Otherwise, use the default reasoning\n",
        "        if \"reasoning\" in ADDITIONAL_PARAMS_DICT:\n",
        "            # User specified reasoning in section 4b, use it (will replace default)\n",
        "            reasoning_config = ADDITIONAL_PARAMS_DICT[\"reasoning\"].copy()\n",
        "            \n",
        "            # Rilevamento del tipo di modello per usare il parametro corretto\n",
        "            model_lower = model_name.lower()\n",
        "            supports_effort = any(keyword in model_lower for keyword in [\n",
        "                \"grok\", \"o1\", \"o3\", \"gpt-5\", \"openai/o1\", \"openai/o3\", \"openai/gpt-5\", \"x-ai\"\n",
        "            ])\n",
        "            supports_max_tokens = any(keyword in model_lower for keyword in [\n",
        "                \"gemini\", \"claude\", \"anthropic\", \"qwen\"\n",
        "            ])\n",
        "            \n",
        "            has_effort = \"effort\" in reasoning_config\n",
        "            has_max_tokens = \"max_tokens\" in reasoning_config\n",
        "            \n",
        "            # Se entrambi sono impostati, scegliamo quello corretto per il modello\n",
        "            if has_effort and has_max_tokens:\n",
        "                if supports_effort:\n",
        "                    # Modello supporta effort, rimuoviamo max_tokens\n",
        "                    del reasoning_config[\"max_tokens\"]\n",
        "                    print(f\"   ‚ÑπÔ∏è Modello {model_name} supporta 'effort', rimosso 'max_tokens'\")\n",
        "                elif supports_max_tokens:\n",
        "                    # Modello supporta max_tokens, manteniamo max_tokens e rimuoviamo effort\n",
        "                    # (l'utente ha impostato max_tokens che √® quello corretto per questo modello)\n",
        "                    effort_val = reasoning_config[\"effort\"]\n",
        "                    del reasoning_config[\"effort\"]\n",
        "                    print(f\"   ‚ÑπÔ∏è Modello {model_name} supporta 'max_tokens', rimosso 'effort' (mantenuto max_tokens: {reasoning_config['max_tokens']})\")\n",
        "                else:\n",
        "                    # Modello sconosciuto, manteniamo effort (pi√π comune)\n",
        "                    del reasoning_config[\"max_tokens\"]\n",
        "                    print(f\"   ‚ö†Ô∏è Modello {model_name} sconosciuto, usando 'effort' (rimosso 'max_tokens')\")\n",
        "            \n",
        "            # Se abbiamo solo effort ma il modello supporta solo max_tokens\n",
        "            elif has_effort and supports_max_tokens and not supports_effort:\n",
        "                effort_val = reasoning_config[\"effort\"]\n",
        "                del reasoning_config[\"effort\"]\n",
        "                \n",
        "                if effort_val == \"none\":\n",
        "                    reasoning_config[\"max_tokens\"] = 0\n",
        "                    print(f\"   ‚ÑπÔ∏è Convertito effort: 'none' ‚Üí max_tokens: 0 per modello {model_name}\")\n",
        "                else:\n",
        "                    # Stima basata su effort\n",
        "                    effort_to_tokens = {\"high\": 2000, \"medium\": 1000, \"low\": 400, \"minimal\": 200}\n",
        "                    reasoning_config[\"max_tokens\"] = effort_to_tokens.get(effort_val, 1000)\n",
        "                    print(f\"   ‚ÑπÔ∏è Convertito effort: '{effort_val}' ‚Üí max_tokens: {reasoning_config['max_tokens']} per modello {model_name}\")\n",
        "            \n",
        "            # Se abbiamo solo max_tokens ma il modello supporta solo effort\n",
        "            elif has_max_tokens and supports_effort and not supports_max_tokens:\n",
        "                max_tokens_val = reasoning_config[\"max_tokens\"]\n",
        "                del reasoning_config[\"max_tokens\"]\n",
        "                \n",
        "                if max_tokens_val == 0:\n",
        "                    reasoning_config[\"effort\"] = \"none\"\n",
        "                    print(f\"   ‚ÑπÔ∏è Convertito max_tokens: 0 ‚Üí effort: 'none' per modello {model_name}\")\n",
        "                else:\n",
        "                    # Per valori > 0, usiamo \"medium\" come default\n",
        "                    reasoning_config[\"effort\"] = \"medium\"\n",
        "                    print(f\"   ‚ÑπÔ∏è Convertito max_tokens: {max_tokens_val} ‚Üí effort: 'medium' per modello {model_name}\")\n",
        "            \n",
        "            # Ricostruiamo ADDITIONAL_PARAMS_DICT con la configurazione modificata\n",
        "            modified_params = ADDITIONAL_PARAMS_DICT.copy()\n",
        "            modified_params[\"reasoning\"] = reasoning_config\n",
        "            data = deep_merge_dict(data, modified_params)\n",
        "        else:\n",
        "            # User specified other params but not reasoning, merge default reasoning + custom params\n",
        "            data = deep_merge_dict(data, default_reasoning)\n",
        "            data = deep_merge_dict(data, ADDITIONAL_PARAMS_DICT)\n",
        "    else:\n",
        "        # No custom params from section 4b, use default reasoning\n",
        "        data = deep_merge_dict(data, default_reasoning)\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.post(OPENROUTER_API_ENDPOINT, headers=headers, json=data, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        choices = result.get(\"choices\")\n",
        "        if choices and len(choices) > 0 and choices[0].get(\"message\") and choices[0][\"message\"].get(\"content\"):\n",
        "            return choices[0][\"message\"][\"content\"].strip()\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è Warning ({purpose}): Could not extract content. Response: {result}\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_msg = f\"\\n‚ùå Error ({purpose}) API call to {model_name}: {e}\"\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            error_msg += f\". Status: {e.response.status_code}, Body: {e.response.text}\"\n",
        "        print(error_msg)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Unexpected error during {purpose} API call ({model_name}): {e}\")\n",
        "        return None\n",
        "\n",
        "def get_llm_classification(query_text, model_name, api_key):\n",
        "    \"\"\"Gets classification using the predefined template with structured messages.\"\"\"\n",
        "    try:\n",
        "        # Access global template defined in Cell 4\n",
        "        prompt_template = CLASSIFICATION_PROMPT_TEMPLATE.format(input_text=\"\") # Format template without the query\n",
        "\n",
        "        # Structure messages for potential caching\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt_template.strip(), \"cache_control\": {\"type\": \"ephemeral\"}}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": query_text.strip()}]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    except KeyError:\n",
        "        print(f\"\\n‚ùå Error: CLASSIFICATION_PROMPT_TEMPLATE missing '{{input_text}}'. Check Cell 4.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error formatting classification prompt: {e}\")\n",
        "        return None\n",
        "    return call_openrouter_llm(messages, model_name, api_key, purpose=\"classification\")\n",
        "\n",
        "def get_semantic_similarity_score(llm_output, ground_truth, model_name, api_key):\n",
        "    \"\"\"Gets similarity score using the predefined template.\"\"\"\n",
        "    if not isinstance(llm_output, str) or not isinstance(ground_truth, str) or not llm_output or not ground_truth:\n",
        "        return 0.0\n",
        "    try:\n",
        "        # Access global template defined in Cell 4\n",
        "        prompt = SIMILARITY_PROMPT_TEMPLATE.format(ground_truth_text=ground_truth, llm_result_text=llm_output)\n",
        "    except KeyError:\n",
        "        print(f\"\\n‚ùå Error: SIMILARITY_PROMPT_TEMPLATE missing placeholder(s). Check Cell 4.\")\n",
        "        return 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error formatting similarity prompt: {e}\")\n",
        "        return 0.0\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}] # Keep simple structure for similarity\n",
        "    raw_score_text = call_openrouter_llm(messages, model_name, api_key, purpose=\"similarity\")\n",
        "    if raw_score_text is None:\n",
        "        print(\"   Similarity scoring failed (API error).\")\n",
        "        return 0.0\n",
        "    # Robust Score Parsing\n",
        "    try:\n",
        "        score = float(raw_score_text.strip())\n",
        "        return max(0.0, min(100.0, score))\n",
        "    except ValueError:\n",
        "        match = re.search(r'\\b(\\d{1,3}(?:\\.\\d+)?)\\b', raw_score_text)\n",
        "        if match:\n",
        "            try:\n",
        "                score = float(match.group(1))\n",
        "                print(f\"   [Sim Score Parsed: {score} from '{raw_score_text}']\", end='')\n",
        "                return max(0.0, min(100.0, score))\n",
        "            except ValueError: pass\n",
        "        print(f\"   Similarity scoring failed: Could not parse number from '{raw_score_text}'.\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"‚úÖ Helper functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLP9hXKvWgUe",
        "outputId": "8542a75a-6451-4a1a-a054-ccb91ccc3f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Funzione principale di valutazione 'run_evaluation' pronta ad essere chiamata.\n"
          ]
        }
      ],
      "source": [
        "#@title 6. Define Main Evaluation Logic\n",
        "\n",
        "def run_evaluation(input_csv_path, output_csv_path, class_model, sim_model, api_key, delay):\n",
        "    \"\"\"\n",
        "    Esegue il ciclo completo di valutazione utilizzando la configurazione definita nelle celle precedenti.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüöÄ Inizio il processo di valutazione...\")\n",
        "    print(\"---\")\n",
        "\n",
        "    # --- Validate API Key ---\n",
        "    if not api_key:\n",
        "        print(\"‚ùå CRITICAL ERROR: la chiave API √® mancante. Non posso procedere.\")\n",
        "        return None # Stop execution\n",
        "\n",
        "    # --- Load Input Data ---\n",
        "    try:\n",
        "        input_df = pd.read_csv(input_csv_path)\n",
        "        required_cols = ['input', 'ground_truth']\n",
        "        input_df.columns = [col.lower().strip() for col in input_df.columns] # Normalize column names\n",
        "        missing_cols = [col for col in required_cols if col not in input_df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Il file CSV '{input_csv_path}' mancano delle colonne richieste: {', '.join(missing_cols)}\")\n",
        "        print(f\"üëç Caricato {len(input_df)} righe da '{input_csv_path}'. Colonne: {list(input_df.columns)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: il file non √® stato trovato in '{input_csv_path}'. Hai caricato il file?\")\n",
        "        return None\n",
        "    except ValueError as ve:\n",
        "        print(f\"‚ùå Error: {ve}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore nel caricamento del file CSV '{input_csv_path}': {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- Run Evaluations ---\n",
        "    results_list = []\n",
        "    total_items = len(input_df)\n",
        "\n",
        "    for index, row in input_df.iterrows():\n",
        "        item_num = index + 1\n",
        "        print(f\"üîÑ Processo query {item_num}/{total_items}: \", end='')\n",
        "\n",
        "        original_query = str(row.get('input', '')).strip()\n",
        "        ground_truth = str(row.get('ground_truth', '')).strip()\n",
        "\n",
        "        if not original_query:\n",
        "            print(\" Saltato - query vuota.\")\n",
        "            llm_classification = \"INPUT_ERROR\"\n",
        "            exact_match = False\n",
        "            similarity_score = 0.0\n",
        "        else:\n",
        "            # 1. Classification\n",
        "            print(\" Classificazione...\", end='')\n",
        "            llm_classification = get_llm_classification(original_query, class_model, api_key)\n",
        "            if llm_classification is None:\n",
        "                print(\" Classificazione fallita.\", end='')\n",
        "                llm_classification = \"CLASSIFICATION_API_ERROR\"\n",
        "                exact_match = False\n",
        "                similarity_score = 0.0\n",
        "            else:\n",
        "                llm_classification = str(llm_classification)\n",
        "                print(f\" Got '{llm_classification}'.\", end='')\n",
        "                # 2. Exact Match\n",
        "                exact_match = (llm_classification == ground_truth)\n",
        "                print(f\" Match: {exact_match}.\", end='')\n",
        "                # 3. Similarity Score\n",
        "                print(\" Scoring Similarity...\", end='')\n",
        "                similarity_score = get_semantic_similarity_score(llm_classification, ground_truth, sim_model, api_key)\n",
        "                print(f\" Score: {similarity_score:.2f}.\", end='')\n",
        "\n",
        "        # Store Result\n",
        "        results_list.append({\n",
        "            'original_query': original_query,\n",
        "            'ground_truth': ground_truth,\n",
        "            'classification': llm_classification,\n",
        "            'match': exact_match,\n",
        "            'semantic_similarity': round(similarity_score, 2)\n",
        "        })\n",
        "        print(\"\") # Newline after processing each item\n",
        "        time.sleep(delay) # Apply delay\n",
        "\n",
        "    # --- Save Results ---\n",
        "    if results_list:\n",
        "        results_df = pd.DataFrame(results_list)\n",
        "        try:\n",
        "            results_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "            print(f\"\\n--- ‚úÖ Valutazione Completata ---\")\n",
        "            print(f\"Risultati salvati in '{output_csv_path}'. Puoi scaricarli dalla scheda Files o il browser dovrebbe proporre di scaricare il file.\")\n",
        "            # --- Calculate and Print Statistics ---\n",
        "            total_queries = len(results_df)\n",
        "            exact_matches = results_df['match'].sum()\n",
        "            partial_matches = total_queries - exact_matches\n",
        "            avg_similarity = results_df['semantic_similarity'].mean()\n",
        "\n",
        "            print(\"\\n--- Statistiche di valutazione ---\")\n",
        "            print(f\"Totale query processate: {total_queries}\")\n",
        "            print(f\"Query con corrispondenza al 100%: {exact_matches}\")\n",
        "            print(f\"Query con corrispondenza <100%: {partial_matches}\")\n",
        "            print(f\"Stima Similarit√† semantica media: {avg_similarity:.2f}\")\n",
        "            # restituisce l'output\n",
        "            return output_csv_path # Return filename on success\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Errore nel salvataggio dei risultati in '{output_csv_path}': {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"\\n--- ‚ö†Ô∏è Valutazione Completata ---\")\n",
        "        print(\"Attenzione: nessun risultato generato. Controlla il file di input e i log.\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Funzione principale di valutazione 'run_evaluation' pronta ad essere chiamata.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "P-w2FjzTWsDh",
        "outputId": "40f183d9-4505-4163-ac2f-06e4cc5a4cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Controlli pre-esecuzione passati. Inizio valutazione...\n",
            "\n",
            "üöÄ Inizio il processo di valutazione...\n",
            "---\n",
            "üëç Caricato 325 righe da '/content/test-300-martino.csv'. Colonne: ['input', 'ground_truth']\n",
            "üîÑ Processo query 1/325:  Classificazione... Got 'Cantine,Franciacorta'. Match: True. Scoring Similarity... Score: 100.00.\n",
            "üîÑ Processo query 2/325:  Classificazione... Got 'Cantine,Franciacorta'. Match: True. Scoring Similarity... Score: 100.00.\n",
            "üîÑ Processo query 3/325:  Classificazione... Got 'Cantine,Franciacorta'. Match: True. Scoring Similarity... Score: 100.00.\n",
            "üîÑ Processo query 4/325:  Classificazione... Got 'Cantine,Franciacorta'. Match: True. Scoring Similarity... Score: 100.00.\n",
            "üîÑ Processo query 5/325:  Classificazione..."
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4152372129.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# --- Call the main function ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     output_file = run_evaluation(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0minput_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_CSV_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moutput_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_CSV_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3191141322.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(input_csv_path, output_csv_path, class_model, sim_model, api_key, delay)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# 1. Classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Classificazione...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mllm_classification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_llm_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mllm_classification\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Classificazione fallita.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-351922011.py\u001b[0m in \u001b[0;36mget_llm_classification\u001b[0;34m(query_text, model_name, api_key)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_openrouter_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurpose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_semantic_similarity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-351922011.py\u001b[0m in \u001b[0;36mcall_openrouter_llm\u001b[0;34m(messages, model_name, api_key, purpose, timeout)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOPENROUTER_API_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title 7. ‚ñ∂Ô∏è Eseguiamo la valutazione\n",
        "\n",
        "# importiamo il time e facciamo partire il cronomentro\n",
        "import time # Import the time module\n",
        "start_time = time.time() # Use time.time() to get the timestamp\n",
        "\n",
        "# controlliamo che tutto sia pronto per l'esecuzione\n",
        "# --- Final Check Before Running ---\n",
        "if 'FINAL_API_KEY' in locals() and FINAL_API_KEY and \\\n",
        "   'CLASSIFICATION_MODEL' in locals() and CLASSIFICATION_MODEL and \\\n",
        "   'SIMILARITY_MODEL' in locals() and SIMILARITY_MODEL and \\\n",
        "   'INPUT_CSV_PATH' in locals() and INPUT_CSV_PATH and \\\n",
        "   'OUTPUT_CSV_PATH' in locals() and OUTPUT_CSV_PATH and \\\n",
        "   'DELAY_SECONDS' in locals() and DELAY_SECONDS >= 0:\n",
        "\n",
        "    print(\"Controlli pre-esecuzione passati. Inizio valutazione...\")\n",
        "    #print(CLASSIFICATION_PROMPT_TEMPLATE)\n",
        "    #print(SIMILARITY_PROMPT_TEMPLATE)\n",
        "\n",
        "    # --- Call the main function ---\n",
        "    output_file = run_evaluation(\n",
        "        input_csv_path=INPUT_CSV_PATH,\n",
        "        output_csv_path=OUTPUT_CSV_PATH,\n",
        "        class_model=CLASSIFICATION_MODEL,\n",
        "        sim_model=SIMILARITY_MODEL,\n",
        "        api_key=FINAL_API_KEY,\n",
        "        delay=DELAY_SECONDS\n",
        "    )\n",
        "\n",
        "    # --- Optional: Offer download link ---\n",
        "    if output_file and os.path.exists(output_file):\n",
        "        try:\n",
        "            print(f\"\\n‚¨áÔ∏è Tentativo di attivare il download per '{os.path.basename(output_file)}'...\")\n",
        "            files.download(output_file)\n",
        "        except Exception as e:\n",
        "            print(f\"   Non √® stato possibile attivare il download automatico: {e}\")\n",
        "            print(f\"   Scarica '{output_file}' manualmente dalla scheda Files.\")\n",
        "    elif output_file:\n",
        "         print(f\"\\nNota: il file di output '{output_file}' era previsto ma non trovato per il download.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ùå Esecuzione annullata: una o pi√π configurazioni critiche mancano.\")\n",
        "    print(\"   Assicurati di avere:\")\n",
        "    print(\"   1. Inserito la chiave API (cella 2 & 3).\")\n",
        "    print(\"   2. Configurato i modelli, i percorsi dei file e i prompt (cella 4).\")\n",
        "    print(\"   3. Caricato il file CSV di input, attenzione ai nomi delle colonne!\")\n",
        "    print(\"   Riavvia le celle sopra se necessario.\")\n",
        "\n",
        "#@title üîî Final Notification Sound\n",
        "\n",
        "import requests\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "try:\n",
        "    # Using a more common format like MP3 might be slightly more compatible, but OGG usually works.\n",
        "    url = \"https://upload.wikimedia.org/wikipedia/commons/0/0c/Meow_domestic_cat.ogg\"\n",
        "    #url = \"https://interactive-examples.mdn.mozilla.net/media/cc0-audio/t-rex-roar.mp3\" # Example MP3\n",
        "\n",
        "    # Define a custom User-Agent header\n",
        "    headers = {\n",
        "        \"User-Agent\": \"MyColabScript/1.0 (https://colab.research.google.com/; MyEmail@example.com)\"\n",
        "    }\n",
        "\n",
        "    audio_response = requests.get(url, headers=headers, timeout=10) # Pass headers to the request\n",
        "    audio_response.raise_for_status() # Check for HTTP errors\n",
        "    audio_content = audio_response.content\n",
        "\n",
        "    print(\"segnale audio...\")\n",
        "    # Explicitly display the Audio object\n",
        "    display(Audio(audio_content, autoplay=True)) # Added autoplay=True\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"‚ùå Errore nel recupero del segnale audio: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore nel riprodurre il segnale audio: {e}\")\n",
        "\n",
        "# fermiamo il timer e stampiamo i secondi\n",
        "end_time = time.time() # Use time.time() to get the timestamp\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tempo di esecuzione: {elapsed_time:.2f} secondi\")\n",
        "# e lo stampiamo anche in minuti\n",
        "print(f\"Tempo di esecuzione: {elapsed_time/60:.2f} minuti\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
